import TipConfiguredDataConnectorOverview from '../components/_tip_configured_data_connector_overview.mdx'
import PartNameTheDataConnector from '../components/_part_name_the_data_connector.mdx'
import PartDataConnectorKeysOverview from '../components/_part_data_connector_keys_overview.mdx'
import TipCustomDataConnectorModuleName from '../components/_tip_custom_data_connector_module_name.mdx'
import PartFilesystemBaseDirectory from '../components/_part_base_directory_for_filesystem.mdx'

<TipConfiguredDataConnectorOverview />

<PartNameTheDataConnector data_connector_name="name_of_my_configured_data_connector" />

At this point, your configuration should look like:

```python name="spark Datasource configuration up to adding an empty configured data connector"
```

<PartDataConnectorKeysOverview data_connector_type="ConfiguredAssetFilesystemDataConnector" data_connector_name="name_of_my_configured_data_connector" runtime={false} batch_spec_passthrough={true} glob_directive={false} />

For this example, you will be using the `ConfiguredAssetFilesystemDataConnector` as your `class_name`.  This is a subclass of the `ConfiguredAssetDataConnector` that is specialized to support filesystem Execution Engines, such as the `SparkDFExecutionEngine`.  This key/value entry will therefore look like:

```python name="configured data connector define class_name as ConfiguredAssetFilesystemDataConnector"
```

<TipCustomDataConnectorModuleName />

<PartFilesystemBaseDirectory />

With these values added, along with blank dictionaries for `assets` and `batch_spec_passthrough`, your full configuration should now look like:

```python name="configured spark datasource_config up to adding empty assets and batch_spec_passthrough dictionaries"
```